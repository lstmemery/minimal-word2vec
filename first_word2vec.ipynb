{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Adapted from this gist: https://gist.github.com/mbednarski/da08eb297304f7a66a3840e857e060a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first few sentences of Frankenstein\n",
    "corpus = \"\"\"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise \n",
    "which you have regarded with such evil forebodings. I arrived here yesterday, and my first task is to \n",
    "assure my dear sister of my welfare and increasing confidence in the success of my undertaking.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rejoice', 'undertaking.', 'confidence', 'here', 'you']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_corpus = [word.strip() for word in corpus.split(' ')]\n",
    "vocabulary = list(set(split_corpus)) # Token should only appear once\n",
    "vocabulary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_indices = list(range(len(vocabulary))) # Each word gets an ordinal\n",
    "vocabulary_size = len(vocabulary_indices)\n",
    "word_to_index = dict(zip(vocabulary, vocabulary_indices))  # Map the word to the ordinal\n",
    "word_to_index['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2 # How many words ahead and behind to look\n",
    "corpus_size = len(split_corpus)\n",
    "\n",
    "\n",
    "index_pairs = []\n",
    "for position, word in enumerate(split_corpus):\n",
    "    window_minimum = max(position - WINDOW_SIZE, 0)\n",
    "    window_maximum = min(position + WINDOW_SIZE + 1, corpus_size)\n",
    "    \n",
    "    for window_position in range(window_minimum, window_maximum):\n",
    "        if position != window_position: # The word itself can't be a context word \n",
    "            context_index = word_to_index[split_corpus[window_position]]\n",
    "            index_pairs.append((context_index, word_to_index[word]))\n",
    "\n",
    "index_pairs = np.array(index_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONS = 5 # What's the size of each vector?\n",
    "LEARNING_RATE = 0.001 # How fast do you want to update these vectors?\n",
    "NUMBER_OF_EPOCHES = 101 # How many times should the model see the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(position):\n",
    "    one_hot_vector = torch.zeros(vocabulary_size).float()\n",
    "    one_hot_vector[position] = 1.0\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputEmbeddingLayer = Variable(\n",
    "    torch.randn( # Random numbers from the standard distribution\n",
    "        EMBEDDING_DIMENSIONS, vocabulary_size\n",
    "    ).float(), \n",
    "    requires_grad=True # This matrix will update\n",
    ")\n",
    "OutputEmbeddingLayer = Variable(torch.randn(vocabulary_size, EMBEDDING_DIMENSIONS).float(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.403818273544312\n",
      "5.057663822174073\n",
      "4.79267865607613\n",
      "4.580320412234256\n",
      "4.4055736340974505\n",
      "4.259079512796904\n",
      "4.134421344807273\n",
      "4.026941872897901\n",
      "3.9331591380269906\n",
      "3.850423762672826\n",
      "3.7767033024838095\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUMBER_OF_EPOCHES):\n",
    "    epoch_loss = 0\n",
    "    for context, target in index_pairs:\n",
    "        input_vector = Variable(one_hot_encoding(context)).float()\n",
    "        ground_truth = Variable(torch.from_numpy(np.array([target])).long()) # The ordinal encoding of the word\n",
    "        \n",
    "        input_embedding = torch.matmul(InputEmbeddingLayer, input_vector)\n",
    "        output_embedding = torch.matmul(OutputEmbeddingLayer, input_embedding)\n",
    "        \n",
    "        log_softmax = F.log_softmax( # Taking the log of the softmax for optimization reasons\n",
    "            output_embedding, \n",
    "            dim=0, # Calculate column-wise\n",
    "        )\n",
    "        loss = F.nll_loss( # The negative-log likelihood: also for optimization reasons\n",
    "            log_softmax.unsqueeze(0), # Transform from a vector to a matrix\n",
    "            ground_truth,\n",
    "        )\n",
    "        epoch_loss += loss.item() # Check track of the total loss\n",
    "        \n",
    "        loss.backward() # Back-propagate (take the derivative of everything)\n",
    "        \n",
    "        InputEmbeddingLayer.data -= LEARNING_RATE * InputEmbeddingLayer.grad.data # Update the embedding layers\n",
    "        OutputEmbeddingLayer.data -= LEARNING_RATE * OutputEmbeddingLayer.grad.data\n",
    "        \n",
    "        InputEmbeddingLayer.grad.data.zero_() # Reset the gradients for the next context-word pair\n",
    "        OutputEmbeddingLayer.grad.data.zero_()\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch_loss / len(index_pairs))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
